This repository contains the code and models used in my master's thesis, titled "Automated Behavioral Coding in Child and Adolescent Psychiatry using Artificial Intelligence". The objective of this project was to develop a machine learning model capable of recognizing human emotions through multimodal data, specifically by combining audio, visual, and textual cues.

**Methodology**

The project employs a multimodal approach, integrating three different types of data:

- Audio: Extracting emotional features from speech.

- Visual: Analyzing facial expressions from video frames.

- Textual: Understanding sentiments from text transcripts.

The features from the modalities were extracted using deep learning architectures from self-supervised models. The fusion of these modalities was achieved using a late-fusion technique to ensure the model could leverage the strengths of each data type effectively.

**Key Components**

Feature Extraction: Techniques used to extract meaningful features from each modality.

Model Architecture: Description of the neural network architectures used.

Training and Evaluation: Methods for training the model and evaluating its performance.
